{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-graded Assignment: Эксперименты с моделью\n",
    "\n",
    "На прошлой неделе вы поучаствовали в соревновании на kaggle и, наверняка, большинство успешно справилось с прохождением baseline, а значит пора двигаться дальше - заняться оптимизацией модели, провести серию экспериментов и построить сильное финальное решения.\n",
    "\n",
    "В этом задании вам нужно провести ряд эскпериментов, оценить качество полученных в процессе экспериментирования моделей и выбрать лучшее решение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание будет оцениваться на основании загруженного jupyther notebook и развернутых ответов на поставленные вопросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "from sklearn.model_selection import learning_curve,StratifiedKFold\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3052.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>vr93T2a</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>xwM2aC7IdeMC0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>6hQ9lNX</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>55YFVY9</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>zCkv</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>e4lqvY0</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>686.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>MAz3HNj</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9  Var10  ...  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN  3052.0   NaN   NaN   NaN    NaN  ...   \n",
       "1   NaN   NaN   NaN   NaN   NaN  1813.0   7.0   NaN   NaN    NaN  ...   \n",
       "2   NaN   NaN   NaN   NaN   NaN  1953.0   7.0   NaN   NaN    NaN  ...   \n",
       "3   NaN   NaN   NaN   NaN   NaN  1533.0   7.0   NaN   NaN    NaN  ...   \n",
       "4   NaN   NaN   NaN   NaN   NaN   686.0   7.0   NaN   NaN    NaN  ...   \n",
       "\n",
       "    Var221   Var222      Var223  Var224  Var225  Var226   Var227  \\\n",
       "0  Al6ZaUT  vr93T2a  LM8l689qOp     NaN     NaN    fKCe  02N6s8f   \n",
       "1     oslk  6hQ9lNX  LM8l689qOp     NaN    ELof    xb3V     RAYp   \n",
       "2     zCkv  catzS2D  LM8l689qOp     NaN     NaN    FSa2     ZI9m   \n",
       "3     oslk  e4lqvY0  LM8l689qOp     NaN     NaN    xb3V     RAYp   \n",
       "4     oslk  MAz3HNj  LM8l689qOp     NaN     NaN    WqMG     RAYp   \n",
       "\n",
       "          Var228  Var229  Var230  \n",
       "0  xwM2aC7IdeMC0     NaN     NaN  \n",
       "1        55YFVY9    mj86     NaN  \n",
       "2  ib5G6X1eUxUn6    mj86     NaN  \n",
       "3  F2FyR07IdsN7I     NaN     NaN  \n",
       "4  F2FyR07IdsN7I     NaN     NaN  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data=pd.read_csv(\"orange_small_churn_data.csv\")\n",
    "name=['target']\n",
    "target=pd.read_csv(\"orange_small_churn_labels.csv\",names=name)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop the columns with 100% of missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 212)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop=[]\n",
    "for i in df_data.columns:\n",
    "    if df_data.isna().sum()[i]/df_data.shape[0]==1:\n",
    "#         print(i,df_data.isna().sum()[i]/df_data.shape[0])\n",
    "        to_drop.append(i)\n",
    "df_data.drop(columns=to_drop,inplace=True)\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating the hold-out dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, hold_out, y_train, y_test = train_test_split(df_data, target['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Based on expert judgment the treshold=75% was selected__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 76)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop_75=[]\n",
    "for i in X_train.columns:\n",
    "    if X_train.isna().sum()[i]/X_train.shape[0]>0.75:\n",
    "        to_drop_75.append(i)\n",
    "X_train.drop(columns=to_drop_75,inplace=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for future testing we create the copy of train and test samples\n",
    "X_train_no_prep=X_train.copy()\n",
    "hold_out_no_prep=hold_out.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "X_train.loc[:,'Var6':'Var189'] = X_train.loc[:,'Var6':'Var189'].fillna(X_train.loc[:,'Var6':'Var189'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the categorical features for which the number of unique values in the training and test samples is the same\n",
    "train={}\n",
    "for i in X_train.loc[:,'Var192':'Var229'].columns:\n",
    "    train[i]=len(X_train[i].unique())\n",
    "test={}\n",
    "for i in hold_out.loc[:,'Var192':'Var229'].columns:\n",
    "    test[i]=len(hold_out[i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the following columns were found:  \n",
    "__'Var194','Var201','Var204','Var205','Var208','Var210','Var211','Var218','Var221','Var223','Var225','Var226',\n",
    "          'Var227','Var229'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing categorical data\n",
    "categ_col=['Var194','Var201','Var204','Var205','Var208','Var210','Var211','Var218','Var221','Var223','Var225','Var226',\n",
    "          'Var227','Var229']\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train[categ_col]=X_train[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train[categ_col])\n",
    "X_train_no_categ=X_train.drop(columns=categ_col)\n",
    "X_train_fin=pd.concat([X_train_no_categ,dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final hold-out-sample\n",
    "hold_out.loc[:,'Var6':'Var189'] = hold_out.loc[:,'Var6':'Var189'].fillna(hold_out.loc[:,'Var6':'Var189'].mean())\n",
    "categ_col=['Var194','Var201','Var204','Var205','Var208','Var210','Var211','Var218','Var221','Var223','Var225','Var226',\n",
    "          'Var227','Var229']\n",
    "col_to_drop=[]\n",
    "for i in test.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "hold_out.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "hold_out[categ_col]=hold_out[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(hold_out[categ_col])\n",
    "hold_out_no_cat=hold_out.drop(columns=categ_col)\n",
    "hold_out_fin=pd.concat([hold_out_no_cat,dummy],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Начнем с простого. Давайте оценим как много объектов действительно нужно для построения качественной модели. Для обучения доступна достаточно большая выборка и может так оказаться, что начиная с некоторого момента рост размера обучающей выборки перестает влиять на качество модели. Постройте кривые обучения, обучая модель на выборках разного размера начиная с небольшого количество объектов в обучающей выборке и постепенно наращивая её размер с некоторым шагом. Обратите внимание на `sklearn.model_selection.learning_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22053a64bc8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "train_sizes, train_scores, test_scores = learning_curve(clf, X_train_fin, y_train, train_sizes=np.linspace(0.1,1.0,10),\n",
    "                                                        cv=StratifiedKFold(n_splits=3, shuffle=True),\n",
    "                                                        scoring='roc_auc', n_jobs=-1)\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.grid(True)\n",
    "plt.plot(train_sizes, train_scores.mean(axis = 1), 'b',  label='train')\n",
    "plt.plot(train_sizes, test_scores.mean(axis = 1), 'r-',  label='hold-out')\n",
    "plt.ylim((0.65, 1.0))\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion: The learning curve continues to decrease as the number of objects grows, but the test curve grows significantly up to 7500 and then continues to grow weakly up to 17500 objects. \n",
    "It can be concluded that 12500-17500 objects of the training sample are enough to build a high-quality model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Часто несбалансированные по классам выборки приводят к различным проблемам при обучении моделей. Давайте попробуем по-разному обработать выборку, поиграть с распределением объектов по классам и сделать выводы о том, как соотношение классов влияет на качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1\\. Задайте веса объектам так, чтобы соотношение классов с учетом весов объектов изменилось. Попробуйте не менее трёх различных вариантов весов. Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    0.926406\n",
       " 1    0.073594\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the __scale_pos_weight__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 0.734541115233449,\n",
       " 1.5: 0.7342709357831954,\n",
       " 2.5: 0.7316413710125286,\n",
       " 4.0: 0.7335700885715983,\n",
       " 8.5: 0.7379852530213905,\n",
       " 12.5: 0.7293619865030997,\n",
       " 19.0: 0.7305089341699845,\n",
       " 37.5: 0.7219346756197726}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {}\n",
    "for weight in [1.0, 1.5, 2.5, 4.0, 8.5, 12.5, 19.0, 37.5]:\n",
    "    clf = xgb.XGBClassifier( random_state=9,scale_pos_weight=weight)\n",
    "    res[weight]=cross_val_score(clf,  X_train_fin, y_train, scoring='roc_auc', \n",
    "                                cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(res.keys()), list(res.values()),label='score')\n",
    "plt.xlabel('weight')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion: Imbalance towards class -1 ( 8.5) has showed the best result__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2\\. Примените к выборке технологию undersampling: для этого нужно убрать из обучения некоторое количество объектов большего класса таким образом, чтобы соотношение классов изменилось. Попробуйте не менее трёх различных вариантов undersampling (варианты могут отличаться как по количество отфильтрованных объектов, так и по принципу выборка объектов для отсеивания из выборки). Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-RandomUnderSampler\n",
    "# 2-EditedNearestNeighbours\n",
    "# 3-NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "under_firts= NeighbourhoodCleaningRule()\n",
    "under_second=EditedNearestNeighbours(sampling_strategy=\"not minority\")\n",
    "under_third=NeighbourhoodCleaningRule()\n",
    "X_res1, y_res1 = under_firts.fit_resample(X_train_fin, y_train)\n",
    "X_res2, y_res2 = under_second.fit_resample(X_train_fin, y_train)\n",
    "X_res3, y_res3 = under_third.fit_resample(X_train_fin, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC with using RandomUnderSampler = 0.7340507685103966\n",
      "ROC-AUC with using EditedNearestNeighbours = 0.7445370214010408\n",
      "ROC-AUC with using NeighbourhoodCleaningRule = 0.7382427744482323\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print('ROC-AUC with using RandomUnderSampler = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_res1, y_res1, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n",
    "print('ROC-AUC with using EditedNearestNeighbours = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_res2, y_res2, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n",
    "print('ROC-AUC with using NeighbourhoodCleaningRule = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_res3, y_res3, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: All of three methods have shown the similar results, but the EditedNearestNeighbours method has the highest ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Теперь перейдем к работе с признаками. Ранее вы реализовали несколько стратегий для обработки пропущенных значений. Сравните эти стратегии между собой с помощью оценки качества моделей кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка пропущенных значений сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill missings with 3 different approaches:  \n",
    "1.__mean__  \n",
    "2.__median__  \n",
    "3.__zero filling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC with using mean filling = 0.7330860686128542\n"
     ]
    }
   ],
   "source": [
    "# MEAN\n",
    "X_train_mean=X_train_no_prep.copy()\n",
    "\n",
    "# Numeric columns\n",
    "X_train_mean.loc[:,'Var6':'Var189'] = X_train_mean.loc[:,'Var6':'Var189'].fillna(X_train_mean.loc[:,'Var6':'Var189'].mean())\n",
    "# preprocessing categorical data\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_mean.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train_mean[categ_col]=X_train_mean[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train_mean[categ_col])\n",
    "X_train_mean_no_categ=X_train_mean.drop(columns=categ_col)\n",
    "X_train_mean_fin=pd.concat([X_train_mean_no_categ,dummy],axis=1)\n",
    "\n",
    "# Modelling\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print('ROC-AUC with using mean filling = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_train_mean_fin, y_train, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC with using median filling = 0.7270283148804491\n"
     ]
    }
   ],
   "source": [
    "X_train_median=X_train_no_prep.copy()\n",
    "X_train_median.loc[:,'Var6':'Var189'] = X_train_median.loc[:,'Var6':'Var189'].fillna(X_train_median.loc[:,'Var6':'Var189'].median())\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_median.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train_median[categ_col]=X_train_median[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train_median[categ_col])\n",
    "X_train_median_no_categ=X_train_median.drop(columns=categ_col)\n",
    "X_train_median_fin=pd.concat([X_train_median_no_categ,dummy],axis=1)\n",
    "\n",
    "# Modelling\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print('ROC-AUC with using median filling = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_train_median_fin, y_train, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC with using zero filling = 0.7348443990850791\n"
     ]
    }
   ],
   "source": [
    "X_train_zero=X_train_no_prep.copy()\n",
    "X_train_zero.loc[:,'Var6':'Var189'] = X_train_zero.loc[:,'Var6':'Var189'].fillna(0)\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_zero.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train_zero[categ_col]=X_train_zero[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train_zero[categ_col])\n",
    "X_train_zero_no_categ=X_train_zero.drop(columns=categ_col)\n",
    "X_train_zero_fin=pd.concat([X_train_zero_no_categ,dummy],axis=1)\n",
    "\n",
    "# Modelling\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print('ROC-AUC with using zero filling = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_train_zero_fin, y_train, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: The filling missings with zero values has showed the highest ROC-AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Также вы уже реализовали несколько стратегий для обработки категориальных признаков. Сравните эти стратегии между собой с помощью оценки качества моделей по кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка категориальных признаков сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill missings with 3 different approaches:   \n",
    "1.__get_dummies ( OneHotEncoder)__  \n",
    "2.__OrdinalEncoder__  \n",
    "3.__HashingEncoder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC with get_dummies = 0.7350061592196688\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder\n",
    "X_train_mean=X_train_no_prep.copy()\n",
    "\n",
    "# Numeric columns\n",
    "X_train_mean.loc[:,'Var6':'Var189'] = X_train_mean.loc[:,'Var6':'Var189'].fillna(X_train_mean.loc[:,'Var6':'Var189'].mean())\n",
    "# preprocessing categorical data\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_mean.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train_mean[categ_col]=X_train_mean[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train_mean[categ_col])\n",
    "X_train_mean_no_categ=X_train_mean.drop(columns=categ_col)\n",
    "X_train_mean_fin=pd.concat([X_train_mean_no_categ,dummy],axis=1)\n",
    "\n",
    "# Modelling\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print('ROC-AUC with get_dummies = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_train_mean_fin, y_train, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC with OrdinalEncoder = 0.7342740369254847\n"
     ]
    }
   ],
   "source": [
    "# OrdinalEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "X_train_mean=X_train_no_prep.copy()\n",
    "\n",
    "# Numeric columns\n",
    "X_train_mean.loc[:,'Var6':'Var189'] = X_train_mean.loc[:,'Var6':'Var189'].fillna(X_train_mean.loc[:,'Var6':'Var189'].mean())\n",
    "# preprocessing categorical data\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_mean.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "\n",
    "X_train_mean[categ_col]=X_train_mean[categ_col].fillna('-100')\n",
    "enc_1=OrdinalEncoder()\n",
    "enc_1.fit(X_train_mean[categ_col])\n",
    "X_train_mean[categ_col]=enc_1.transform(X_train_mean[categ_col])\n",
    "\n",
    "# Modelling\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print('ROC-AUC with OrdinalEncoder = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_train_mean, y_train, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC with HashingEncoder = 0.732228055111432\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import HashingEncoder\n",
    "X_train_mean=X_train_no_prep.copy()\n",
    "\n",
    "# Numeric columns\n",
    "X_train_mean.loc[:,'Var6':'Var189'] = X_train_mean.loc[:,'Var6':'Var189'].fillna(X_train_mean.loc[:,'Var6':'Var189'].mean())\n",
    "# preprocessing categorical data\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_mean.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "\n",
    "X_train_mean[categ_col]=X_train_mean[categ_col].fillna('-100')\n",
    "enc_2=HashingEncoder()\n",
    "enc_2.fit(X_train_mean[categ_col])\n",
    "X_train_mean[categ_col]=enc_1.transform(X_train_mean[categ_col])\n",
    "\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print('ROC-AUC with HashingEncoder = {}'.format(cross_val_score(clf, \n",
    "                                                                          X_train_mean, y_train, \n",
    "                                                                          scoring='roc_auc', \n",
    "                                                                          cv=StratifiedKFold(n_splits=3, shuffle=True)).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: All of three methods have shown the similar results, but the OneHotEncoding method has the highest ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Все ли признаки оказались полезными для построения моделей? Проведите процедуру отбора признаков, попробуйте разные варианты отбора (обратите внимание на модуль `sklearn.feature_selection`). Например, можно выбрасывать случайные признаки или строить отбор на основе l1-регуляризации - отфильтровать из обучения признаки, которые получат нулевой вес при построении регрессии с l1-регуляризацией (`sklearn.linear_model.Lasso`). И всегда можно придумать что-то своё=) Попробуйте как минимум 2 различные стратегии, сравните результаты. Помог ли отбор признаков улучшить качество модели? Поясните свой ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's making feature selection with 2 different approaches:  \n",
    "1.with model XGBoost  \n",
    "2.sklearn.linear_model.Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after feature selection=31\n",
      "Roc Auc with feature selection= 0.7347090011430782\n"
     ]
    }
   ],
   "source": [
    "X_train_mean=X_train_no_prep.copy()\n",
    "\n",
    "# Numeric columns\n",
    "X_train_mean.loc[:,'Var6':'Var189'] = X_train_mean.loc[:,'Var6':'Var189'].fillna(X_train_mean.loc[:,'Var6':'Var189'].mean())\n",
    "# preprocessing categorical data\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_mean.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train_mean[categ_col]=X_train_mean[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train_mean[categ_col])\n",
    "X_train_mean_no_categ=X_train_mean.drop(columns=categ_col)\n",
    "X_train_mean_fin=pd.concat([X_train_mean_no_categ,dummy],axis=1)\n",
    "\n",
    "\n",
    "thershold=0.01\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "clf.fit(X_train_mean_fin,y_train)\n",
    "new_vars = X_train_mean_fin.columns[clf.feature_importances_ > thershold]\n",
    "df_after_selection = X_train_mean_fin[new_vars]\n",
    "print(f'Number of columns after feature selection={len(new_vars)}')\n",
    "print('Roc Auc with feature selection=',cross_val_score(clf, df_after_selection, y_train, scoring ='roc_auc',\n",
    "                                                    cv=StratifiedKFold(n_splits=3, shuffle=True)).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after feature selection=18\n",
      "Roc Auc with feature selection= 0.6506250794174592\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "X_train_mean=X_train_no_prep.copy()\n",
    "\n",
    "# Numeric columns\n",
    "X_train_mean.loc[:,'Var6':'Var189'] = X_train_mean.loc[:,'Var6':'Var189'].fillna(X_train_mean.loc[:,'Var6':'Var189'].mean())\n",
    "# preprocessing categorical data\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_mean.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train_mean[categ_col]=X_train_mean[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train_mean[categ_col])\n",
    "X_train_mean_no_categ=X_train_mean.drop(columns=categ_col)\n",
    "X_train_mean_fin=pd.concat([X_train_mean_no_categ,dummy],axis=1)\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_train_mean_fin,y_train)\n",
    "\n",
    "new_vars = X_train_mean_fin.columns[lasso.coef_ != 0]\n",
    "df_after_selection = X_train_mean_fin[new_vars]\n",
    "\n",
    "clf = xgb.XGBClassifier(random_state=9)\n",
    "print(f'Number of columns after feature selection={len(new_vars)}')\n",
    "print('Roc Auc with feature selection=',cross_val_score(clf, df_after_selection, y_train, scoring ='roc_auc',\n",
    "                                                    cv=StratifiedKFold(n_splits=3, shuffle=True)).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: Feature selection using by feature_importances_ has shown the best ROC-AUC. But model with all features has a little better performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Подберите оптимальные параметры модели. Обратите внимание, что в зависимости от того, как вы обработали исходные данные, сделали ли балансировку классов, сколько объектов оставили в обучающей выборке и др. оптимальные значения параметров могут меняться. Возьмите наилучшее из ваших решений на текущий момент и проведите процедуру подбора параметров модели (обратите внимание на `sklearn.model_selection.GridSearchCV`) Как подбор параметров повлиял на качество модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use __RandomizedSearchCV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.6, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 2, 'colsample_bytree': 0.8}\n",
      "XGBClassifier(colsample_bytree=0.8, gamma=2, min_child_weight=5, random_state=9,\n",
      "              subsample=0.6)\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0,0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6,7]\n",
    "        } \n",
    "model = xgb.XGBClassifier(random_state=9)\n",
    "clf=RandomizedSearchCV(model,params,scoring='roc_auc')\n",
    "clf.fit(X_train_fin, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc Auc with feature selection= 0.7369371232176976\n"
     ]
    }
   ],
   "source": [
    "xg_reg_mod = xgb.XGBClassifier(colsample_bytree=0.8, gamma=2, min_child_weight=5, random_state=9,\n",
    "              subsample=0.6)\n",
    "print('Roc Auc with feature selection=',cross_val_score(xg_reg_mod, X_train_fin, y_train, scoring ='roc_auc',\n",
    "                                                    cv=StratifiedKFold(n_splits=3, shuffle=True)).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Предложите методику оценки того, какие признаки внесли наибольший вклад в модель (например, это могут быть веса в случае регрессии, а также большое количество моделей реализуют метод `feature_importances_` - оценка важности признаков). На основе предложенной методики проанализируйте, какие признаки внесли больший вклад в модель, а какие меньший?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg_mod = xgb.XGBClassifier(colsample_bytree=0.8, gamma=2, min_child_weight=5, random_state=9,\n",
    "              subsample=0.6)\n",
    "xg_reg_mod.fit(X_train_fin, y_train)\n",
    "# Top 10 features\n",
    "features={}\n",
    "for i,j in  zip(X_train_fin.columns[:10],xg_reg_mod.feature_importances_[:10] ):\n",
    "    features[i]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Var6': 0.013897232,\n",
       " 'Var7': 0.010904791,\n",
       " 'Var13': 0.017002296,\n",
       " 'Var21': 0.012705123,\n",
       " 'Var22': 0.012122465,\n",
       " 'Var24': 0.005196493,\n",
       " 'Var25': 0.014400118,\n",
       " 'Var28': 0.010388267,\n",
       " 'Var35': 0.010203031,\n",
       " 'Var38': 0.010378593}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напоследок давайте посмотрим на объекты. На каких объектах достигается наибольшая ошибка классификации? Есть ли межу этими объектами что-то общее? Видны ли какие-либо закономерности? Предположите, почему наибольшая ошибка достигается именно на этих объектах. В данном случае \"наибольшую\" ошибку можно понимать как отнесение объекта с чужому классу с большой долей уверенности (с высокой вероятностью)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg_mod = xgb.XGBClassifier(colsample_bytree=0.8, gamma=2, min_child_weight=5, random_state=9,\n",
    "              subsample=0.6)\n",
    "xg_reg_mod.fit(X_train_fin, y_train)\n",
    "for_analysis = X_train_fin.copy()\n",
    "for_analysis[\"target\"] = y_train\n",
    "for_analysis['prediction'] = xg_reg_mod.predict_proba(X_train_fin)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var38</th>\n",
       "      <th>...</th>\n",
       "      <th>Var227_nIGXDli</th>\n",
       "      <th>Var227_nIGjgSB</th>\n",
       "      <th>Var227_vJ_w8kB</th>\n",
       "      <th>Var229_-100</th>\n",
       "      <th>Var229_am7c</th>\n",
       "      <th>Var229_mj86</th>\n",
       "      <th>Var229_oJmt</th>\n",
       "      <th>Var229_sk2h</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>38498</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.56433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>230.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.606597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6984</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>925.000000</td>\n",
       "      <td>14.00000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.787130e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.580339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17147</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>415.280000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.923828e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.567471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28807</td>\n",
       "      <td>707.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>589.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.296346e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.566946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37549</td>\n",
       "      <td>1326.551049</td>\n",
       "      <td>6.80378</td>\n",
       "      <td>1241.19882</td>\n",
       "      <td>234.149088</td>\n",
       "      <td>289.503577</td>\n",
       "      <td>4.56433</td>\n",
       "      <td>96.919017</td>\n",
       "      <td>223.738787</td>\n",
       "      <td>0.711384</td>\n",
       "      <td>2.563561e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.560103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Var6     Var7       Var13       Var21       Var22     Var24  \\\n",
       "38498   560.000000  0.00000     0.00000    0.000000    0.000000   4.56433   \n",
       "6984    280.000000  0.00000     0.00000  740.000000  925.000000  14.00000   \n",
       "17147    42.000000  0.00000     0.00000   40.000000   50.000000   2.00000   \n",
       "28807   707.000000  7.00000     4.00000  128.000000  160.000000   0.00000   \n",
       "37549  1326.551049  6.80378  1241.19882  234.149088  289.503577   4.56433   \n",
       "\n",
       "            Var25       Var28      Var35         Var38  ...  Var227_nIGXDli  \\\n",
       "38498    0.000000  230.080000   0.000000  0.000000e+00  ...               0   \n",
       "6984   240.000000   20.080000   0.000000  4.787130e+06  ...               0   \n",
       "17147   24.000000  415.280000   5.000000  4.923828e+06  ...               0   \n",
       "28807   48.000000  589.600000  10.000000  3.296346e+06  ...               0   \n",
       "37549   96.919017  223.738787   0.711384  2.563561e+06  ...               0   \n",
       "\n",
       "       Var227_nIGjgSB  Var227_vJ_w8kB  Var229_-100  Var229_am7c  Var229_mj86  \\\n",
       "38498               0               0            1            0            0   \n",
       "6984                0               0            1            0            0   \n",
       "17147               0               0            1            0            0   \n",
       "28807               0               0            1            0            0   \n",
       "37549               0               0            1            0            0   \n",
       "\n",
       "       Var229_oJmt  Var229_sk2h  target  prediction  \n",
       "38498            0            0      -1    0.606597  \n",
       "6984             0            0      -1    0.580339  \n",
       "17147            0            0      -1    0.567471  \n",
       "28807            0            0      -1    0.566946  \n",
       "37549            0            0      -1    0.560103  \n",
       "\n",
       "[5 rows x 220 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_analysis[for_analysis['target'] == -1].sort_values('prediction', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var22</th>\n",
       "      <th>Var24</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var35</th>\n",
       "      <th>Var38</th>\n",
       "      <th>...</th>\n",
       "      <th>Var227_nIGXDli</th>\n",
       "      <th>Var227_nIGjgSB</th>\n",
       "      <th>Var227_vJ_w8kB</th>\n",
       "      <th>Var229_-100</th>\n",
       "      <th>Var229_am7c</th>\n",
       "      <th>Var229_mj86</th>\n",
       "      <th>Var229_oJmt</th>\n",
       "      <th>Var229_sk2h</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4309</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4712.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>176.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3762.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4135</td>\n",
       "      <td>4879.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1684.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>186.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76404.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4012</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3908.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>223.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>369966.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21429</td>\n",
       "      <td>854.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4752.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>200.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92916.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7761</td>\n",
       "      <td>7476.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9136.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>166.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1792152.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 220 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Var6  Var7   Var13  Var21   Var22  Var24  Var25   Var28  Var35  \\\n",
       "4309   1246.0   7.0  4712.0  180.0   225.0    4.0  128.0  176.56    0.0   \n",
       "4135   4879.0  35.0  1684.0  568.0   710.0   32.0   72.0  186.64    0.0   \n",
       "4012   2226.0   7.0  3908.0  504.0   630.0   14.0  296.0  223.36    0.0   \n",
       "21429   854.0  14.0  4752.0  200.0   250.0    0.0   40.0  200.00    0.0   \n",
       "7761   7476.0  28.0  9136.0  980.0  1225.0   46.0  472.0  166.56    0.0   \n",
       "\n",
       "           Var38  ...  Var227_nIGXDli  Var227_nIGjgSB  Var227_vJ_w8kB  \\\n",
       "4309      3762.0  ...               0               0               0   \n",
       "4135     76404.0  ...               0               0               0   \n",
       "4012    369966.0  ...               0               0               0   \n",
       "21429    92916.0  ...               0               0               0   \n",
       "7761   1792152.0  ...               0               0               0   \n",
       "\n",
       "       Var229_-100  Var229_am7c  Var229_mj86  Var229_oJmt  Var229_sk2h  \\\n",
       "4309             0            1            0            0            0   \n",
       "4135             1            0            0            0            0   \n",
       "4012             0            0            1            0            0   \n",
       "21429            0            0            1            0            0   \n",
       "7761             0            1            0            0            0   \n",
       "\n",
       "       target  prediction  \n",
       "4309        1    0.007792  \n",
       "4135        1    0.010194  \n",
       "4012        1    0.010593  \n",
       "21429       1    0.011038  \n",
       "7761        1    0.011813  \n",
       "\n",
       "[5 rows x 220 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_analysis[for_analysis['target'] == 1].sort_values('prediction', ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: Too many columns have been deleted so there are many errors. It seems to need to use dimensionality reduction methods (e.x. PCA, tSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. По итогам проведенных экспериментов постройте финальную решение - модель с наилучшим качеством. Укажите, какие преобразования данных, параметры и пр. вы выбрали для построения финальной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model:XGBClassifier(colsample_bytree=0.8, min_child_weight=10, random_state=9,\n",
      "              subsample=0.6)\n"
     ]
    }
   ],
   "source": [
    "X_train_mean=X_train_no_prep.copy()\n",
    "\n",
    "# Numeric columns\n",
    "X_train_mean.loc[:,'Var6':'Var189'] = X_train_mean.loc[:,'Var6':'Var189'].fillna(X_train_mean.loc[:,'Var6':'Var189'].mean())\n",
    "# preprocessing categorical data\n",
    "col_to_drop=[]\n",
    "for i in train.keys():\n",
    "    if i not in categ_col:\n",
    "        col_to_drop.append(i)\n",
    "X_train_mean.drop(columns=col_to_drop,inplace=True)\n",
    "\n",
    "# creating the final training sample\n",
    "X_train_mean[categ_col]=X_train_mean[categ_col].fillna(-100)\n",
    "dummy=pd.get_dummies(X_train_mean[categ_col])\n",
    "X_train_mean_no_categ=X_train_mean.drop(columns=categ_col)\n",
    "X_train_mean_fin=pd.concat([X_train_mean_no_categ,dummy],axis=1)\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0,0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6,7,10]\n",
    "        } \n",
    "model = xgb.XGBClassifier(random_state=9)\n",
    "clf=RandomizedSearchCV(model,params,scoring='roc_auc')\n",
    "clf.fit(X_train_fin, y_train)\n",
    "print(f'Final model:{clf.best_estimator_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First of all it is necessary to fill missings with mean values;\n",
    "* Secondly categorical features were processed by OneHotEncoding;\n",
    "* None of undersampling methods was used;\n",
    "* Final model was build on numeric features which have less than 75% missings and categorical features for which the number of unique values in the training and test samples is the same;\n",
    "* Finally the parameyers of the model were selected by RandomizedSearchCV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Подумайте, можно ли еще улучшить модель? Что для этого можно сделать? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* It seems to to need to use dimensionality reduction methods (e.x. PCA, tSNE);\n",
    "* It is possible to use another methods of processing categorical data ( Binary method, target-based methods);\n",
    "* Using the another models ( RandomForest with tuning parameters or Catboost)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
